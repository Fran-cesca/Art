{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48581096",
   "metadata": {},
   "source": [
    "#NOME FIGO\n",
    "Art and history of art are no sealed compartments: they are heavily inter-dependent with social, political, economic factors, which in turn influence our very perception of what art is.\n",
    "\n",
    "Cultural institutions and museums in particular play a fundamental role in this intertwined dynamics: through their selection activity, they have the potential to shape the public understanding of arts and its modifications throughout time. \n",
    "In some way, what makes into museums makes into history of art.\n",
    "\n",
    "From these considerations stems our analysis: how do external (social, political, economic) factors influence the perception of art and its history?\n",
    "A way to investigate it is by looking at the greatest and most representative museums around the world, and at their acquisition policies and campaigns in particular.\n",
    "\n",
    "Our key questions:\n",
    "In which ways have the acquisition campaigns of the major museums in the world changed throughout the years? \n",
    "\n",
    "\n",
    "Our workflow:\n",
    "1. Interrogate WikiData:\n",
    "    - What are the biggest collections around the world?\n",
    "2. Find csv files for some of the major museums.\n",
    "3. Select some representative time slots (both internal and external factors).\n",
    "4. Analyse acquisitions during these time slots for every museum and compare:\n",
    "    a) Difference between different slots in the same museum;\n",
    "    b) Difference between different museums for the same time slot;\n",
    "\n",
    "Our questions:\n",
    "- What was the initial nucleus of each museum? \n",
    "- Internal survey: Is there a significant date or decade for the acquisitions? \n",
    "- External survey: What are the acquisition trends around the Xs/between the x and the y? / What are the acquisition trends within and across these museums? \n",
    "- During these years, who are the most represented makers? What is the most represented gender? What is the most represented movement? What is the most represented nationality? \n",
    "\n",
    "\n",
    "We analysed 5/4 of the (MET, MoMa, N+, Cleveland?, Tate) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368dc5d",
   "metadata": {},
   "source": [
    "Wikidata interrogation: failure.\n",
    "\n",
    "1. What are the largest art collections?\n",
    "\n",
    "SELECT ?museum (COUNT(?work) AS ?works) WHERE {\n",
    "  ?work wdt:P195 ?museum.\n",
    "  ?museum wdt:P31 wd:Q207694\n",
    "  }\n",
    "GROUP BY ?museum \n",
    "ORDER BY DESC(?works)\n",
    "\n",
    "2.  Which were the most visited museums in 2018?\n",
    "\n",
    "SELECT ?museumLabel ?visitors ?year\n",
    "WHERE {\n",
    "  ?museum wdt:P31 wd:Q207694;\n",
    "          wdt:P1705 ?museumLabel;\n",
    "          wdt:P1174 ?visitors;\n",
    "          p:P1174/pq:P585 ?year .\n",
    "FILTER(YEAR(?year) = 2018).\n",
    "}\n",
    "\n",
    "ORDER BY DESC(?visitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4267ffa",
   "metadata": {},
   "source": [
    "Since WikiData was not providing reliable results, we decided to go back to its sources (The Art Newspaper https://www.theartnewspaper.com/) and manually collect data about the most visited museums in the last four years(2018-2022).\n",
    "\n",
    "https://onedrive.live.com/view.aspx?resid=E34DDE1A3F2F2160!138&ithint=file%2cxlsx&authkey=!AN4u-K4bko37iOU\n",
    "    \n",
    "We verified the availability of open datasets for each of the top 20 most visited museums on this GitHub repository (https://github.com/Ambrosiani/museums-on-github), containing a list of museums with GitHub accounts.\n",
    "\n",
    "Our analysis led us to the decision to focus on four museums:\n",
    "- Tate Modern, London\n",
    "- MoMa, NY\n",
    "- Met, NY\n",
    "- National Gallery of Art, Washington DC\n",
    "\n",
    "**Info generale sui musei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc975d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456bdd0",
   "metadata": {},
   "source": [
    "# Creating dataframes from online CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ba48f",
   "metadata": {},
   "source": [
    "First: let us create some pandas dataframes containing all needed information: for each Museum, we will integrate different csv files, selecting the data we need for each of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b791fb",
   "metadata": {},
   "source": [
    "## MoMa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2a310a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = pd.read_csv('https://media.githubusercontent.com/media/MuseumofModernArt/collection/master/Artworks.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "artworks = spreadsheet[['Title', 'Artist', 'ConstituentID', 'Nationality', 'BeginDate', 'EndDate', 'Gender', 'Date', 'Medium', 'CreditLine', 'Classification', 'Department', 'DateAcquired', 'URL']]\n",
    "artists = pd.read_csv('https://media.githubusercontent.com/media/MuseumofModernArt/collection/master/Artists.csv')\n",
    "artists[\"ConstituentID\"] = artists[\"ConstituentID\"].astype(str)\n",
    "MoMa = pd.merge(artworks,artists[['ConstituentID', 'Wiki QID']],on='ConstituentID', how='left')\n",
    "MoMa.rename(columns = {'ConstituentID':'Id', 'BeginDate':'BirthDate', 'EndDate':'DeathDate'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca614fd4",
   "metadata": {},
   "source": [
    "## Tate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1dd54728",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = pd.read_csv('https://raw.githubusercontent.com/tategallery/collection/master/artwork_data.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "artworks = spreadsheet[['artist', 'artistId', 'title', 'medium', 'creditLine', 'year', 'acquisitionYear', 'url']]\n",
    "artworks.rename(columns = {'artistId':'id'}, inplace = True)\n",
    "artworks.id = artworks.id.astype(str)\n",
    "artists = pd.read_csv('https://raw.githubusercontent.com/tategallery/collection/master/artist_data.csv')\n",
    "artists[\"id\"] = artists[\"id\"].astype(str)\n",
    "Tate = pd.merge(artworks,artists[['id', 'gender', 'yearOfBirth', 'yearOfDeath']], on='id', how='left')\n",
    "Tate.rename(columns = {'artist':'Artist', 'id':'Id', 'title':'Title', 'yearOfBirth':'BirthDate', 'yearOfDeath':'DeathDate', 'medium':'Medium', 'creditLine':'CreditLine', 'year':'Date', 'acquisitionYear':'DateAcquired', 'url':'URL', 'gender':'Gender'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e8108",
   "metadata": {},
   "source": [
    "## Met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62bab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francesca\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3172: DtypeWarning: Columns (5,7,10,11,12,13,14,34,35,36,37,38,39,40,41,42,43,44,45,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\Francesca\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:4438: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "spreadsheet = pd.read_csv('https://media.githubusercontent.com/media/metmuseum/openaccess/master/MetObjects.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "Met = spreadsheet[['AccessionYear', 'Title', 'Culture', 'Artist Display Name', 'Artist Nationality', 'Artist Begin Date', 'Artist End Date', 'Artist Gender', 'Artist Wikidata URL', 'Object End Date', 'Medium', 'Credit Line', 'Classification', 'Link Resource', 'Object Wikidata URL']]\n",
    "Met.rename(columns = {'Artist Display Name':'Artist', 'id':'Id', 'Artist Begin Date':'BirthDate', 'Artist End Date':'DeathDate', 'Credit Line':'CreditLine', 'Object End Date':'Date', 'AccessionYear':'DateAcquired', 'Artist Wikidata URL':'Wiki QID', 'Artist Gender':'Gender', 'Link Resource':'URL', 'Artist Nationality':'Nationality'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618155b",
   "metadata": {},
   "source": [
    "## Nga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1f86f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = pd.read_csv('https://raw.githubusercontent.com/NationalGalleryOfArt/opendata/main/data/objects.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "Nga = spreadsheet[['accessionnum', 'title', 'endyear', 'medium', 'attribution', 'creditline', 'classification']]\n",
    "Nga.rename(columns = {'attribution':'Artist', 'id':'Id', 'title':'Title', 'medium':'Medium', 'creditline':'CreditLine', 'endyear':'Date', 'accessionnum':'DateAcquired', 'classification':'Classification', 'Object End Date':'Date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480357e",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3bfc1",
   "metadata": {},
   "source": [
    "For each dataframe, we now want to do some cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04038b",
   "metadata": {},
   "source": [
    "## Cleaning dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "07cdbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDatesMoMa(date):\n",
    "    if '-' in date:\n",
    "        splitted = date.split('-')\n",
    "        date = ' '.join(splitted) \n",
    "    if '/' in date:\n",
    "        splitted = date.split('/')\n",
    "        date = ' '.join(splitted) \n",
    "    if ',' in date:\n",
    "        splitted = date.split(',')\n",
    "        date = ' '.join(splitted) \n",
    "    if '.' in date:\n",
    "        splitted = date.split('.')\n",
    "        date = ' '.join(splitted) \n",
    "        \n",
    "    x = re.search(\"\\d{4}\", date)\n",
    "    if x:\n",
    "        date = x.group()\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105b549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDates(date):\n",
    "    if '.' in date:\n",
    "        date = date.split('.')[0]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03c7ca",
   "metadata": {},
   "source": [
    "### Cleaning artworks' creation and acquisition dates for each museum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05200bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francesca\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:4459: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "C:\\Users\\FRANCE~1\\AppData\\Local\\Temp/ipykernel_10260/2886862666.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Met[\"Date\"] = Met[\"Date\"].astype(str)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleanDates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FRANCE~1\\AppData\\Local\\Temp/ipykernel_10260/2886862666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanDates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanDates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleanDates' is not defined"
     ]
    }
   ],
   "source": [
    "Met.fillna('None', inplace=True)\n",
    "Met[\"Date\"] = Met[\"Date\"].astype(str)\n",
    "Met[\"Date\"] = Met[\"Date\"].apply(cleanDates)\n",
    "Met[\"DateAcquired\"] = Met[\"DateAcquired\"].astype(str)\n",
    "Met[\"DateAcquired\"] = Met[\"DateAcquired\"].apply(cleanDates)\n",
    "Met.to_csv('Met.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b05e509",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MoMa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FRANCE~1\\AppData\\Local\\Temp/ipykernel_2056/4160744149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMoMa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanDatesMoMa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMoMa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DateAcquired\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanDatesMoMa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MoMa' is not defined"
     ]
    }
   ],
   "source": [
    "MoMa.fillna(value='None', inplace=True)\n",
    "MoMa[\"Date\"] = MoMa[\"Date\"].astype(str)\n",
    "MoMa[\"Date\"] = MoMa[\"Date\"].apply(cleanDatesMoMa)\n",
    "MoMa[\"DateAcquired\"] = MoMa[\"DateAcquired\"].astype(str)\n",
    "MoMa[\"DateAcquired\"] = MoMa[\"DateAcquired\"].apply(cleanDatesMoMa)\n",
    "MoMa.to_csv('MoMa.csv')\n",
    "\n",
    "\n",
    "#values =['(n d )','TBD','nd','c  196?','no date','date of publicati','New York','Not available','Various','Various','unknown','Unknown','n d ','n d ','n d','n  d ','Unkown','TBC']\n",
    "#Tate['Date']=Tate['Date'].replace(['(n d )','c','TBD','nd','c  196?','no date','date of publicati','New York','Not available','Various','Various','unknown','Unknown','n d ','n d ','n d','n  d ','Unkown','TBC'],value='None')\n",
    "Tate.fillna(value='None', inplace=True)\n",
    "Tate[\"Date\"] = Tate[\"Date\"].astype(str)\n",
    "Tate[\"Date\"] = Tate[\"Date\"].apply(cleanDates)\n",
    "Tate[\"DateAcquired\"] = Tate[\"DateAcquired\"].astype(str)\n",
    "Tate[\"DateAcquired\"] = Tate[\"DateAcquired\"].apply(cleanDates)\n",
    "Tate.to_csv('Tate.csv')\n",
    "\n",
    "Met.fillna('None', inplace=True)\n",
    "Met[\"Date\"] = Met[\"Date\"].astype(str)\n",
    "Met[\"Date\"] = Met[\"Date\"].apply(cleanDates)\n",
    "Met[\"DateAcquired\"] = Met[\"DateAcquired\"].astype(str)\n",
    "Met[\"DateAcquired\"] = Met[\"DateAcquired\"].apply(cleanDates)\n",
    "Met['Wiki QID'] = Met[\"Date\"].astype(str)\n",
    "Met.to_csv('Met.csv')\n",
    "\n",
    "\n",
    "Nga.fillna(value='None', inplace=True)\n",
    "Nga[\"Date\"] = Nga[\"Date\"].astype(str)\n",
    "Nga[\"Date\"] = Nga[\"Date\"].apply(cleanDates)\n",
    "Nga[\"DateAcquired\"] = Nga[\"DateAcquired\"].astype(str)\n",
    "Nga[\"DateAcquired\"] = Nga[\"DateAcquired\"].apply(cleanDates)\n",
    "Nga.to_csv('Nga.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a7ff1",
   "metadata": {},
   "source": [
    "# Exploring our Museums\n",
    "<br>\n",
    "Now that we have our dataframes, we can explore the four collections.\n",
    "<br>\n",
    "- How many items does each collection contain?\n",
    "- Which timespan do items cover overall?\n",
    "- First and last acquisition date for each museum. Tate's csv last update dates back to 2014.\n",
    "- Total artists' number.\n",
    "- Most represented artist, gender and nationality in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "48e9b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artworks at Moma : 140848\n",
      "Total artworks at Met : 477804\n",
      "Total artworks at Tate : 69201\n",
      "Total artworks at Nga : 138037\n"
     ]
    }
   ],
   "source": [
    "museums=[MoMa, Met, Tate, Nga]\n",
    "names = ['Moma', 'Met', 'Tate', 'Nga']\n",
    "for museum in museums:\n",
    "    selected_rows = museum[~museum['Title'].isnull()]\n",
    "    name = names.pop(0)\n",
    "    print(\"Total artworks at\", name, \":\", len(selected_rows.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453ec9c",
   "metadata": {},
   "source": [
    "# When do artworks date back?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab457e",
   "metadata": {},
   "source": [
    "Artworks' timespan <br>\n",
    "For Met, we use a csv file we cleaned by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb491153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most ancient artwork at Moma dates back to 1768\n",
      "Most recent artwork at Moma dates back to 2022\n",
      "Most ancient artwork at Nga dates back to -490\n",
      "Most recent artwork at Nga dates back to 2021\n",
      "Most ancient artwork at Tate dates back to 1545\n",
      "Most recent artwork at Tate dates back to 2012\n",
      "Most ancient artwork at Met dates back to -240000\n",
      "Most recent artwork at Met dates back to 2022\n"
     ]
    }
   ],
   "source": [
    "museums=['Moma.csv','Nga.csv','Tate.csv', 'metclean2.csv']\n",
    "names = ['Moma', 'Nga','Tate', 'Met']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        years=[]\n",
    "        for item in reader:\n",
    "            if item['Date'] != '' and item['Date'] != 'None'and item['Date'] != '(n d )'and item['Date'] != 'TBD'and item['Date'] != 'nd'and item['Date']!='c  196?' and 'c' not in item['Date'] and item['Date'] != 'no date' and item['Date'] != 'date of publicati' and item['Date'] != 'New York' and item['Date'] != 'Not available' and item['Date'] != 'Various' and item['Date'] != 'Various' and item['Date'] != 'unknown' and 'century' not in item['Date'] and item['Date'] != 'Unknown' and item['Date'] != 'n d ' and '(' not in item['Date'] and item['Date'] != 'n d ' and item['Date'] != 'n d' and item['Date'] != 'n  d ' and item['Date'] != 'Unkown' and item['Date'] != 'TBC':\n",
    "                years.append(item['Date'])\n",
    "        clean = []\n",
    "        for el in years:\n",
    "            clean.append(int(el))  \n",
    "        clean.sort()\n",
    "        name = names.pop(0)\n",
    "    print(\"Most ancient artwork at\", name, \"dates back to\", clean[0])\n",
    "    print(\"Most recent artwork at\", name, \"dates back to\", clean[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96c54b",
   "metadata": {},
   "source": [
    "# When were artworks acquired?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301f54b",
   "metadata": {},
   "source": [
    "## Acquisition' timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "59e54cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First recorderd acquisition MoMa dates back to 1929\n",
      "Last recorded acquisition MoMa dates back to 2022\n",
      "First recorderd acquisition Met dates back to 1870\n",
      "Last recorded acquisition Met dates back to 2022\n",
      "First recorderd acquisition Nga dates back to 1937\n",
      "Last recorded acquisition Nga dates back to 2022\n",
      "First recorderd acquisition Tate dates back to 1823\n",
      "Last recorded acquisition Tate dates back to 2013\n"
     ]
    }
   ],
   "source": [
    "museums=['MoMa.csv','Met.csv', 'Nga.csv', 'Tate.csv']\n",
    "names = ['MoMa','Met', 'Nga', 'Tate']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        acquisitionyears=[]\n",
    "        for item in reader:\n",
    "            if item['DateAcquired'] != 'None' and item['DateAcquired'] != 'Object Number':\n",
    "                acquisitionyears.append(item['DateAcquired'])\n",
    "        acquisitionyears.sort()\n",
    "        name = names.pop(0)\n",
    "    print(\"First recorderd acquisition\", name, \"dates back to\", acquisitionyears[0])\n",
    "    print(\"Last recorded acquisition\", name, \"dates back to\", acquisitionyears[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6550b",
   "metadata": {},
   "source": [
    "# Total artists' number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1dba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanArtistsTate(name):\n",
    "    if ',' in name:\n",
    "        splitted = name.split(',')\n",
    "        name = ''.join(splitted) \n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a7d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TateNew = Tate.copy(deep=True)\n",
    "TateNew[\"Artist\"] = TateNew[\"Artist\"].apply(cleanArtistsTate)\n",
    "TateNew.to_csv(\"TateNew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e3905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of artists at MoMa is 14738\n",
      "Number of artists at Met is 60950\n",
      "Number of artists at Tate is 3281\n",
      "Number of artists at Nga is 16860\n"
     ]
    }
   ],
   "source": [
    "museums=['MoMa.csv', 'Met.csv', 'TateNew.csv', 'Nga.csv']\n",
    "names = ['MoMa', 'Met','Tate', 'Nga']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        artists = set() \n",
    "        for item in reader:\n",
    "            if item['Artist']!= '' and 'Unidentified'not in item['Artist'] and 'Various' not in item['Artist']:\n",
    "                if ',' in item['Artist']:\n",
    "                    item['Artist'] = item['Artist'].split(',')\n",
    "                    for n in range(len(item['Artist'])):\n",
    "                        artist= item['Artist'][n]\n",
    "                        artists.add(artist)\n",
    "                elif '|' in item['Artist']:\n",
    "                    artists_list = item['Artist'].split('|')\n",
    "                    for n in range(len(artists_list)):\n",
    "                        artista= artists_list[n]\n",
    "                        artists.add(artista)\n",
    "                else:\n",
    "                    artists.add(item['Artist'])\n",
    "    name = names.pop(0)\n",
    "    print(\"Number of artists at\", name, \"is\", len(artists) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6401960",
   "metadata": {},
   "source": [
    "Most represented gender in general?\n",
    "Only for MoMa and Tate since other csv files need Wikidata integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5a8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoMaGender = MoMa.drop_duplicates(subset='Artist', keep=\"first\")\n",
    "MoMaGender.to_csv('MoMaGender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef467d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "TateGender = Tate.drop_duplicates(subset='Artist', keep=\"first\")\n",
    "TateGender.to_csv('TateGender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4b4d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Male': 10474, 'Female': 2809}\n",
      "{'Male': 2791, 'Female': 492}\n"
     ]
    }
   ],
   "source": [
    "museums=['MoMaGender.csv', 'TateGender.csv']\n",
    "names = ['MoMa', 'Tate']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        Gender = {'Male':0, 'Female':0}\n",
    "        for item in reader:\n",
    "                if 'Male' in item['Gender']:\n",
    "                    Gender['Male'] += 1\n",
    "                if 'Female' in item['Gender']:\n",
    "                    Gender['Female'] += 1\n",
    "    print(Gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998d930",
   "metadata": {},
   "source": [
    "Most represented nationality in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbeacf",
   "metadata": {},
   "source": [
    "# Nationalities at MoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf85c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMoMaNationality(nationality):\n",
    "    if ('(') or (')') in nationality:\n",
    "        a = nationality.replace('(', '').replace(')', ',')\n",
    "    return a.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430f4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoMaGender = MoMaGender[MoMaGender['Nationality'].notna()]\n",
    "MoMaGender[\"Nationality\"] = MoMaGender[\"Nationality\"].apply(cleanMoMaNationality)\n",
    "MoMaGender.to_csv('MoMaGender.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9570d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Swedish', 'Venezuelan', 'Swiss', 'Icelandic', 'Canadian', 'Japanese', 'Kyrgyz', 'Slovak', 'Portuguese', 'Ghanaian', 'Costa Rican', 'Scottish', 'New Zealander', 'Greek', 'Algerian', 'Iranian', 'Ecuadorian', 'Irish', 'American', 'Brazilian', 'Vietnamese', 'Cambodian', 'Moroccan', 'Taiwanese', 'Zimbabwean', 'Norwegian', 'Kuwaiti', 'Sudanese', 'Malian', 'Uruguayan', 'Bolivian', 'Canadian Inuit', 'Ivorian', 'Tanzanian', 'Bulgarian', 'Puerto Rican', 'Finnish', 'Cypriot', 'Bahamian', 'Colombian', 'Indian', 'Congolese', 'Hungarian', 'Senegalese', 'Singaporean', 'Beninese', 'Chilean', 'Mozambican', 'Argentine', 'Malaysian', 'Namibian', 'Salvadoran', 'Czechoslovakian', 'Ugandan', 'Tunisian', 'Macedonian', 'Dutch', 'Palestinian', 'British', 'Thai', 'Catalan', 'Danish', 'South African', 'Albanian', 'Bosnian', 'Paraguayan', 'Yugoslav', 'Bangladeshi', 'Cameroonian', 'Panamanian', 'Nicaraguan', 'Luxembourger', 'Peruvian', 'Pakistani', 'Israeli', 'Lithuanian', 'Kenyan', 'Croatian', 'Azerbaijani', 'Austrian', 'Lebanese', 'Cuban', 'Welsh', 'Italian', 'Nigerian', 'Slovenian', 'Serbian', 'Romanian', 'Emirati', 'Georgian', 'Korean', 'Burkinabé', 'Latvian', 'French', 'Sierra Leonean', 'Polish', 'Chinese', 'German', 'Guatemalan', 'Iraqi', 'Belgian', 'Afghan', 'Ethiopian', 'Haitian', 'Sahrawi', 'Turkish', 'Ukrainian', 'Australian', 'Egyptian', 'Spanish', 'Russian', 'Czech', 'Coptic', 'Mexican', 'Filipino', 'Estonian', 'Native American'}\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "museums=['MoMaGender.csv']\n",
    "names = ['MoMa']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        nationalities = set() \n",
    "        for item in reader:\n",
    "            if ',' in item['Nationality']:\n",
    "                    item['Nationality'] = item['Nationality'].split(',')\n",
    "                    for n in range(len(item['Nationality'])):\n",
    "                        nationality= item['Nationality'][n].strip()\n",
    "                        nationalities.add(nationality)\n",
    "            else:\n",
    "                    nationalities.add(item['Nationality'])\n",
    "        count_naz= set()\n",
    "        for el in nationalities:\n",
    "            if el != '' and el != 'Nationality unknown' and el != ',' and el != ' ':\n",
    "                count_naz.add(el)\n",
    "                \n",
    "print(count_naz)\n",
    "print(len(count_naz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f6c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Swedish': 123, 'Venezuelan': 68, 'Swiss': 421, 'Icelandic': 21, 'Canadian': 205, 'Japanese': 545, 'Kyrgyz': 1, 'Slovak': 9, 'Portuguese': 16, 'Ghanaian': 4, 'Costa Rican': 2, 'Scottish': 22, 'New Zealander': 10, 'Greek': 13, 'Algerian': 4, 'Iranian': 11, 'Ecuadorian': 4, 'Irish': 24, 'American': 5584, 'Brazilian': 185, 'Vietnamese': 3, 'Cambodian': 1, 'Moroccan': 8, 'Taiwanese': 4, 'Zimbabwean': 5, 'Norwegian': 35, 'Kuwaiti': 1, 'Sudanese': 2, 'Malian': 3, 'Uruguayan': 24, 'Bolivian': 3, 'Canadian Inuit': 3, 'Ivorian': 2, 'Tanzanian': 1, 'Bulgarian': 5, 'Puerto Rican': 6, 'Finnish': 52, 'Cypriot': 1, 'Bahamian': 1, 'Colombian': 57, 'Indian': 39, 'Congolese': 6, 'Hungarian': 86, 'Senegalese': 2, 'Singaporean': 2, 'Beninese': 1, 'Chilean': 68, 'Mozambican': 1, 'Argentine': 150, 'Malaysian': 2, 'Namibian': 2, 'Salvadoran': 2, 'Czechoslovakian': 3, 'Ugandan': 1, 'Tunisian': 2, 'Macedonian': 5, 'Dutch': 288, 'Palestinian': 3, 'British': 962, 'Thai': 7, 'Catalan': 1, 'Danish': 125, 'South African': 68, 'Albanian': 4, 'Bosnian': 9, 'Paraguayan': 3, 'Yugoslav': 1, 'Bangladeshi': 1, 'Cameroonian': 2, 'Panamanian': 2, 'Nicaraguan': 2, 'Luxembourger': 3, 'Peruvian': 37, 'Pakistani': 4, 'Israeli': 77, 'Lithuanian': 5, 'Kenyan': 2, 'Croatian': 47, 'Azerbaijani': 2, 'Austrian': 277, 'Lebanese': 10, 'Cuban': 71, 'Welsh': 4, 'Italian': 517, 'Nigerian': 6, 'Slovenian': 19, 'Serbian': 17, 'Romanian': 27, 'Emirati': 1, 'Georgian': 22, 'Korean': 35, 'Burkinabé': 1, 'Latvian': 13, 'French': 1041, 'Sierra Leonean': 1, 'Polish': 148, 'Chinese': 106, 'German': 1163, 'Guatemalan': 7, 'Iraqi': 2, 'Belgian': 125, 'Afghan': 1, 'Ethiopian': 2, 'Haitian': 16, 'Sahrawi': 1, 'Turkish': 20, 'Ukrainian': 40, 'Australian': 59, 'Egyptian': 14, 'Spanish': 185, 'Russian': 374, 'Czech': 98, 'Coptic': 1, 'Mexican': 159, 'Filipino': 3, 'Estonian': 2, 'Native American': 10}\n"
     ]
    }
   ],
   "source": [
    "museums=['MoMaGender.csv']\n",
    "names = ['MoMa']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        Nationalities ={}\n",
    "        for nationality in count_naz:  \n",
    "            Nationalities[nationality]= 0\n",
    "        for item in reader:\n",
    "            for naz in count_naz:\n",
    "                    if naz in item['Nationality']:\n",
    "                        Nationalities[naz] += 1 \n",
    "    print(Nationalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f6757",
   "metadata": {},
   "source": [
    "# Nationalities at Met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2553270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNazMet(naz):\n",
    "    if ',' in naz:\n",
    "        naz = naz.split(',')[0]\n",
    "    if '(' in naz:\n",
    "        naz = naz.split('(')[0]\n",
    "    if '?' in naz:\n",
    "        naz = naz.replace('?', '')\n",
    "    if '1866–1932' in naz:\n",
    "        naz = naz.replace(' 1866–1932', '')\n",
    "    if 'born' in naz:\n",
    "        naz = naz.replace(' born', '')\n",
    "    if ' and ' in naz:\n",
    "        naz = naz.replace(' and ', '-') \n",
    "    if 'South ' in naz:\n",
    "        naz= naz.replace('South ', '')\n",
    "    if 'Southern ' in naz:\n",
    "        naz= naz.replace('Southern ', '')\n",
    "    if 'Northern ' in naz:\n",
    "        naz= naz.replace('Northern ', '')\n",
    "    if 'West ' in naz:\n",
    "        naz= naz.replace('West ', '')\n",
    "    if 'North ' in naz:\n",
    "        naz= naz.replace('North ', '')\n",
    "    if 'GErman' in naz:\n",
    "        naz= naz.replace('GErman', 'German')\n",
    "    if 'Geman' in naz:\n",
    "        naz= naz.replace('Geman', 'German')\n",
    "    if 'BRitish' in naz:\n",
    "        naz= naz.replace('BRitish', 'British')\n",
    "    if 'Britsh' in naz:\n",
    "        naz= naz.replace('Britsh', 'British')\n",
    "    if 'Fench' in naz:\n",
    "        naz= naz.replace('Fench', 'French')\n",
    "    if 'active in France by 1894' in naz:\n",
    "        naz= naz.replace('active in France by 1894', 'French')\n",
    "    if 'french' == naz:\n",
    "        naz= naz.replace('french', 'French')\n",
    "    if 'France' == naz:\n",
    "        naz= naz.replace('France', 'French')\n",
    "    if ' / ' in naz:\n",
    "        naz= naz.replace(' / ', '-')\n",
    "    if '/' in naz:\n",
    "        naz= naz.replace('/', '-')\n",
    "    if 'possibly ' in naz:\n",
    "        naz= naz.replace('possibly ', '')\n",
    "    if 'probably ' in naz:\n",
    "        naz= naz.replace('probably ', '')\n",
    "    if ' [search purposes only]' in naz:\n",
    "        naz= naz.replace(' [search purposes only]', '')\n",
    "    if 'U.S.' in naz:\n",
    "        naz= naz.replace('U.S.', 'United States')\n",
    "    if 'Nethrlandish' in naz:\n",
    "        naz= naz.replace('Nethrlandish', 'Netherlandish')\n",
    "    if 'Netherlands' in naz:\n",
    "        naz= naz.replace('Netherlands', 'Netherlandish')\n",
    "    if 'German-born sculptor later active in Switzerland' in naz:\n",
    "        naz= naz.replace('German-born sculptor later active in Switzerland', 'German')\n",
    "    if 'Hannover' in naz:\n",
    "        naz= naz.replace('Hannover','German')\n",
    "    if 'German-Solingen' in naz:\n",
    "        naz= naz.replace('German-Solingen','German')\n",
    "    if 'italian' in naz:\n",
    "        naz= naz.replace('italian','Italian')\n",
    "    if 'Italy' in naz:\n",
    "        naz= naz.replace('Italy','Italian')\n",
    "    if 'Dem. Republic of the Congo' == naz:\n",
    "        naz= naz.replace('Dem. Republic of the Congo', 'Democratic Republic of Congo')\n",
    "    if 'Hanoverian' == naz:\n",
    "        naz= naz.replace('Hanoverian', 'German')\n",
    "    if 'Germany' == naz:\n",
    "        naz= naz.replace('Germany', 'German')\n",
    "    if 'Finish' in naz:\n",
    "        naz= naz.replace('Finish', 'Finnish')\n",
    "    if 'Spainish' == naz:\n",
    "        naz= naz.replace('Spainish', 'Spanish')\n",
    "    if 'Central European' in naz:\n",
    "        naz= naz.replace('Central European', 'European')\n",
    "    if 'Chiricahua Apache Native American' in naz:\n",
    "        naz= naz.replace('Chiricahua Apache Native American', 'Native American')\n",
    "    if 'Algonquin family' in naz:\n",
    "        naz= naz.replace('Algonquin family', 'Native American')\n",
    "    if 'Continental-Swiss' in naz:\n",
    "        naz= naz.replace('Continental-Swiss', 'Swiss')\n",
    "    if 'Siena' in naz:\n",
    "        naz= naz.replace('Siena', 'Sienese')\n",
    "    if 'Italian French' in naz:\n",
    "        naz= naz.replace('French Italian', 'Italian French')\n",
    "    if 'Africa' in naz:\n",
    "        naz= naz.replace('Africa', 'African')\n",
    "    if 'Africann' in naz:\n",
    "        naz= naz.replace('Africann', 'African')\n",
    "    if 'Coast Africa' in naz:\n",
    "        naz= naz.replace('Coast Africa', 'African')\n",
    "    if 'New York' in naz:\n",
    "        naz= naz.replace('New York', 'American')\n",
    "    if 'Antelope Valley Washoe' in naz:\n",
    "        naz= naz.replace('Antelope Valley Washoe', 'American')\n",
    "    if 'Venice' in naz:\n",
    "        naz= naz.replace('Venice', 'Venetian')\n",
    "    if 'Dutch' in naz:\n",
    "        naz= naz.replace('Dutch', 'Netherlandish')\n",
    "    \n",
    "    return naz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0ebf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetNew = Met.copy(deep=True)\n",
    "MetNew = MetNew[MetNew['Nationality'].notna()]\n",
    "MetNew = MetNew[MetNew['Nationality'].str.contains(' or ')==False]\n",
    "MetNew = MetNew[MetNew['Nationality'].str.contains('Manufacturer')==False]\n",
    "MetNew = MetNew[MetNew['Nationality'].str.contains('Continental')==False]\n",
    "MetNew[\"Nationality\"] = MetNew[\"Nationality\"].apply(cleanNazMet)\n",
    "MetNew.to_csv(\"MetNew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe3bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'Costa Rican', 'Greek', 'Jamaican', 'British', 'Egyptian', 'Franco-Flemish', 'Greenlandish', 'Kashmir', 'Sienese', 'Flemish', 'American England', 'Norway', 'Chinese', 'Asmat', 'Mexican', 'Nigeria', 'Norwegian', 'Netherlandish-French', 'Guatemalan', 'English', 'Uganda', 'Aachen', 'Hanoverian', 'England', 'Byzantine', 'Malaysian', 'Democratic Republic of Congo', 'Slovakian', 'Venetian', 'Franco-American', 'Armenian-Iranian', 'Romanian', 'British French', 'Italian French', 'Africann', 'Argentinian', 'Native American', 'French American', 'Samoan', 'Netherlandish', 'American German', 'Saint Lucian', 'Congo', 'Scottish', 'New Zealander', 'Afghani', 'French-English', 'Portuguese', 'Sri Lankan', 'French British', 'Serbo-croat', 'Iranian-American', 'American-Canadian', 'Venezuelan', 'Spanish', 'Albanian', 'United States', 'Manchu', 'Malian', 'American Britain', 'Florence', 'European', 'Mexican-American', 'Panamanian', 'Scandinavian', 'French-Netherlandish', 'Colombian', 'Persian', 'Polish', 'African', 'Tunisian', 'Korean', 'American Indian', 'Turkish', 'Swiss American', 'British American', 'Danish-Icelandic', 'German-Swiss', 'Icelandic', 'French Flemish', 'Armenian', 'Puerto Rican', 'Danish', 'Croatian', 'Austrian', 'Belgium', 'Finnish', 'Cuban', 'Bremen', 'Belgian', 'French-Flemish', 'Spanish-Mexican', 'Rumanian', 'Ukranian', 'American', 'France', 'Naples', 'Algerian', 'Yugoslavian', 'Mexico', 'Japanese American', 'Italian', 'Australian', 'Lithuanian', 'Polish Lithunanian', 'Israeli', 'Iran', 'Upper Rhine', 'Czech', 'Islamic', 'Welsh', 'Paris-London', 'Japan', 'Alsatian', 'Belarussian', 'Austrian-German', 'London', 'Prussian', 'French', 'Emilian', 'Estonian', 'Sierra Leone', 'Haida', 'German', 'Pakistani', 'Swedish', 'Bulgarian', 'Spainish', 'Japanese', 'Ecuadorian', 'Denmark', 'Kuwaiti', 'Iraqi Kurdish', 'Salvadoran', 'Lombard', 'Lebanese', 'Canadian', 'Roman', 'Khitan', 'French Italian', 'Hungarian', 'American-French', 'Indian', 'Czechoslovakian', 'Uruguayan', 'Gubbio', 'American-British', 'Paris', 'Franco-Netherlandish', 'German American', 'Georgian', 'Ghanaian', 'Chilean', 'British-Irish', 'Austrian German', 'American Russian', 'Iranian', 'Nigerian', 'Mexican American', 'Austro-Hungarian', 'Netherlandish German', 'Taiwanese', 'Maltese', 'Andorran', 'Cameroonian', 'American Irish', 'Tajikistan', 'Ottoman', 'Dominican', 'Tibetan', 'Bohemian', 'Burgundian', 'Mongolian', 'Swiss', 'Liechtensteiner', 'French Netherlandish', 'American Romanian', 'French-Canadian', 'Ancient Greek', 'Ukrainian', 'Klingenthal', 'Japanese-German', 'Tuscany', 'Brazilian', 'Peruvian', 'Russian', 'Irish', 'Inuit', 'Nigeria-Cameroon', 'Veronese'}\n"
     ]
    }
   ],
   "source": [
    "museums=['MetNew.csv']\n",
    "names = ['Met']\n",
    "for museum in museums:\n",
    "    with open(museum, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        nationalities = set() \n",
    "        for item in reader:\n",
    "            if '|' in item['Nationality']:\n",
    "                    item['Nationality'] = item['Nationality'].split('|')\n",
    "                    for n in range(len(item['Nationality'])):\n",
    "                        nationality= item['Nationality'][n].strip()\n",
    "                        nationalities.add(nationality)\n",
    "            else:\n",
    "                nationalities.add(item['Nationality'].strip())\n",
    "print(nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "262730a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 275417, 'Costa Rican': 1, 'Greek': 170, 'Jamaican': 1, 'British': 18794, 'Egyptian': 10, 'Franco-Flemish': 1, 'Greenlandish': 1, 'Kashmir': 1, 'Sienese': 5, 'Flemish': 1560, 'American England': 1, 'Norway': 1, 'Chinese': 2058, 'Asmat': 2, 'Mexican': 358, 'Nigeria': 22, 'Norwegian': 85, 'Netherlandish-French': 3, 'Guatemalan': 1, 'English': 9, 'Uganda': 1, 'Aachen': 1, 'Hanoverian': 1, 'England': 3, 'Byzantine': 1, 'Malaysian': 31, 'Democratic Republic of Congo': 2, 'Slovakian': 1, 'Venetian': 4, 'Franco-American': 1, 'Armenian-Iranian': 2, 'Romanian': 13, 'British French': 1, 'Italian French': 2, 'Africann': 1, 'Argentinian': 19, 'Native American': 6, 'French American': 1, 'Samoan': 5, 'Netherlandish': 9297, 'American German': 9, 'Saint Lucian': 5, 'Congo': 5, 'Scottish': 412, 'New Zealander': 5, 'Afghani': 1, 'French-English': 2, 'Portuguese': 33, 'Sri Lankan': 1, 'French British': 1, 'Serbo-croat': 1, 'Iranian-American': 1, 'American-Canadian': 3, 'Venezuelan': 15, 'Spanish': 2454, 'Albanian': 1, 'United States': 16, 'Manchu': 1, 'Malian': 23, 'American Britain': 3, 'Florence': 1, 'European': 15, 'Mexican-American': 1, 'Panamanian': 1, 'Scandinavian': 5, 'French-Netherlandish': 33, 'Colombian': 7, 'Persian': 6, 'Polish': 143, 'African': 39, 'Tunisian': 1, 'Korean': 38, 'American Indian': 2, 'Turkish': 38, 'Swiss American': 1, 'British American': 21, 'Danish-Icelandic': 3, 'German-Swiss': 1, 'Icelandic': 4, 'French Flemish': 2, 'Armenian': 5, 'Puerto Rican': 10, 'Danish': 436, 'Croatian': 4, 'Austrian': 1811, 'Belgium': 1, 'Finnish': 141, 'Cuban': 9, 'Bremen': 1, 'Belgian': 476, 'French-Flemish': 1, 'Spanish-Mexican': 1, 'Rumanian': 2, 'Ukranian': 6, 'American': 68655, 'France': 5, 'Naples': 1, 'Algerian': 17, 'Yugoslavian': 4, 'Mexico': 25, 'Japanese American': 22, 'Italian': 24455, 'Australian': 86, 'Lithuanian': 33, 'Polish Lithunanian': 1, 'Israeli': 61, 'Iran': 44, 'Upper Rhine': 1, 'Czech': 268, 'Islamic': 2, 'Welsh': 1, 'Paris-London': 2, 'Japan': 6527, 'Alsatian': 1, 'Belarussian': 1, 'Austrian-German': 1, 'London': 4, 'Prussian': 1, 'French': 41422, 'Emilian': 1, 'Estonian': 1, 'Sierra Leone': 1, 'Haida': 3, 'German': 11579, 'Pakistani': 14, 'Swedish': 294, 'Bulgarian': 1, 'Spainish': 1, 'Japanese': 6526, 'Ecuadorian': 1, 'Denmark': 1, 'Kuwaiti': 3, 'Iraqi Kurdish': 1, 'Salvadoran': 1, 'Lombard': 1, 'Lebanese': 15, 'Canadian': 247, 'Roman': 582, 'Khitan': 1, 'French Italian': 6, 'Hungarian': 249, 'American-French': 37, 'Indian': 226, 'Czechoslovakian': 28, 'Uruguayan': 14, 'Gubbio': 2, 'American-British': 2, 'Paris': 3, 'Franco-Netherlandish': 6, 'German American': 12, 'Georgian': 1, 'Ghanaian': 4, 'Chilean': 18, 'British-Irish': 1, 'Austrian German': 2, 'American Russian': 2, 'Iranian': 42, 'Nigerian': 20, 'Mexican American': 1, 'Austro-Hungarian': 42, 'Netherlandish German': 6, 'Taiwanese': 1, 'Maltese': 1, 'Andorran': 1, 'Cameroonian': 2, 'American Irish': 1, 'Tajikistan': 1, 'Ottoman': 2, 'Dominican': 1, 'Tibetan': 2, 'Bohemian': 1299, 'Burgundian': 2, 'Mongolian': 4, 'Swiss': 1005, 'Liechtensteiner': 1, 'French Netherlandish': 1, 'American Romanian': 1, 'French-Canadian': 5, 'Ancient Greek': 2, 'Ukrainian': 28, 'Klingenthal': 1, 'Japanese-German': 5, 'Tuscany': 4, 'Brazilian': 46, 'Peruvian': 11, 'Russian': 778, 'Irish': 468, 'Inuit': 3, 'Nigeria-Cameroon': 1, 'Veronese': 1}\n"
     ]
    }
   ],
   "source": [
    "with open('MetNew.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        MetNationalities ={}\n",
    "        for nationality in nationalities:  \n",
    "            MetNationalities[nationality]= 0\n",
    "        for item in reader:\n",
    "        \n",
    "            for naz in nationalities:\n",
    "                    if naz in item['Nationality']:\n",
    "                        MetNationalities[naz] += 1 \n",
    "print(MetNationalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390ea4a",
   "metadata": {},
   "source": [
    "# Nationalities at Nga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6285abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NgaNationalities = pd.read_csv('https://raw.githubusercontent.com/NationalGalleryOfArt/opendata/main/data/constituents.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "NgaNationalities = NgaNationalities[['artistofngaobject', 'nationality']]\n",
    "NgaNationalities = NgaNationalities[NgaNationalities['artistofngaobject']==1]\n",
    "NgaNationalities = NgaNationalities[NgaNationalities['nationality'].notna()]\n",
    "NgaNationalities = NgaNationalities[NgaNationalities['nationality'] != 'Unknown']\n",
    "NgaNationalities = NgaNationalities [NgaNationalities ['nationality'].str.contains(' or ')==False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95b8c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNazNga(naz):\n",
    "    if ' (?)' in naz:\n",
    "         naz = naz.replace(' (?)', '')\n",
    "    if 'italian' in naz:\n",
    "         naz = naz.replace('italian', 'Italian')\n",
    "    if '/' in naz:\n",
    "         naz = naz.replace('/', '-')\n",
    "    if '\\u206eItalian' in naz:\n",
    "         naz = naz.replace('\\u206eItalian', 'Italian')\n",
    "    if ' and ' in naz:\n",
    "        naz = naz.replace(' and ', '-')\n",
    "        naz= naz.split('-')\n",
    "        naz= naz[1] + '-' + naz[0]\n",
    "    if 'South ' in naz:\n",
    "        naz= naz.replace('South ', '')\n",
    "    if 'West ' in naz:\n",
    "        naz= naz.replace('West ', '')\n",
    "        \n",
    "      \n",
    "    \n",
    "    return naz   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c4baa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "NgaNationalities[\"nationality\"] = NgaNationalities[\"nationality\"].apply(cleanNazNga)\n",
    "NgaNationalities.to_csv(\"NgaNationalities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3d87bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'British', 'Welsh', 'Franco-Flemish', 'Dalmatian', 'Croatian', 'New Zealander', 'Flemish', 'Dutch', 'Cuban', 'Bosnian', 'Iranian', 'Swiss', 'Spanish', 'Albanian', 'Slovenian', 'Bohemian', 'Argentinean', 'Irish', 'Mexican-American', 'American', 'Persian', 'Slovak', 'Scottish', 'Czech', 'French', 'Netherlandish', 'Argentine', 'Chilean', 'Germany', 'Romanian', 'Greek', 'Yugoslavian', 'Egyptian', 'Scandinavian', 'Belgian', 'Latvian', 'Australian', 'Armenian', 'Hispano-Flemish', 'Genoese', 'Swedish', 'Uruguayan', 'Japanese', 'Mexican', 'Chinese', 'Brazilian', 'Lithuanian', 'Moroccan', 'Bolognese', 'Etruscan', 'Korean', 'Hungarian', 'German', 'New Zealand', 'Turkish', 'Israeli', 'Malian', 'Danish', 'Colombian', 'Nigerian', 'Roman', 'Czech-Israeli', 'Guatemalan', 'Indonesian', 'Austrian', 'Portuguese', 'Peruvian', 'Mosan', 'Italian', 'Nicaraguan', 'African', 'Russian', 'Polish', 'Bavarian', 'Icelandic', 'Veneto-Islamic', 'Canadian', 'English', 'Indian', 'Ukrainian', 'Byzantine', 'Liechtenstein', 'Nepalese', 'Venezuelan', 'Norwegian', 'Taiwanese', 'Latin', 'Martiniquais', 'Finnish', 'Bahamian', 'Ethiopian', 'Bulgarian', 'British-American', 'European'}\n"
     ]
    }
   ],
   "source": [
    "with open('NgaNationalities.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        nationalities = set() \n",
    "        for item in reader:\n",
    "            nationality= item['nationality'].strip()\n",
    "            nationalities.add(nationality)\n",
    "\n",
    "print(nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2b3a4bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'British': 1144, 'Welsh': 10, 'Franco-Flemish': 3, 'Dalmatian': 1, 'Croatian': 2, 'New Zealander': 2, 'Flemish': 251, 'Dutch': 472, 'Cuban': 8, 'Bosnian': 1, 'Iranian': 2, 'Swiss': 157, 'Spanish': 92, 'Albanian': 1, 'Slovenian': 1, 'Bohemian': 11, 'Argentinean': 20, 'Irish': 29, 'Mexican-American': 2, 'American': 6807, 'Persian': 3, 'Slovak': 1, 'Scottish': 55, 'Czech': 177, 'French': 2176, 'Netherlandish': 111, 'Argentine': 2, 'Chilean': 6, 'Germany': 1, 'Romanian': 6, 'Greek': 23, 'Yugoslavian': 14, 'Egyptian': 2, 'Scandinavian': 2, 'Belgian': 52, 'Latvian': 2, 'Australian': 22, 'Armenian': 1, 'Hispano-Flemish': 1, 'Genoese': 1, 'Swedish': 39, 'Uruguayan': 2, 'Japanese': 104, 'Mexican': 37, 'Chinese': 22, 'Brazilian': 16, 'Lithuanian': 2, 'Moroccan': 1, 'Bolognese': 1, 'Etruscan': 1, 'Korean': 5, 'Hungarian': 27, 'German': 1205, 'New Zealand': 1, 'Turkish': 2, 'Israeli': 19, 'Malian': 1, 'Danish': 26, 'Colombian': 2, 'Nigerian': 2, 'Roman': 9, 'Czech-Israeli': 1, 'Guatemalan': 2, 'Indonesian': 1, 'Austrian': 138, 'Portuguese': 5, 'Peruvian': 3, 'Mosan': 2, 'Italian': 1577, 'Nicaraguan': 1, 'African': 8, 'Russian': 96, 'Polish': 64, 'Bavarian': 1, 'Icelandic': 1, 'Veneto-Islamic': 1, 'Canadian': 48, 'English': 126, 'Indian': 7, 'Ukrainian': 1, 'Byzantine': 1, 'Liechtenstein': 1, 'Nepalese': 1, 'Venezuelan': 4, 'Norwegian': 9, 'Taiwanese': 1, 'Latin': 1, 'Martiniquais': 1, 'Finnish': 4, 'Bahamian': 1, 'Ethiopian': 1, 'Bulgarian': 3, 'British-American': 1, 'European': 7}\n"
     ]
    }
   ],
   "source": [
    "with open('NgaNationalities.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        NgaNationalities ={}\n",
    "        for nationality in nationalities:  \n",
    "            NgaNationalities[nationality]= 0\n",
    "        for item in reader:\n",
    "            for naz in nationalities:\n",
    "                    if naz == item['nationality']:\n",
    "                        NgaNationalities[naz] += 1 \n",
    "print(NgaNationalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a74c1",
   "metadata": {},
   "source": [
    "# Nationalities at Tate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8472387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TateArtists = pd.read_csv('https://raw.githubusercontent.com/tategallery/collection/master/artist_data.csv')\n",
    "TateArtists = TateArtists[TateArtists['placeOfBirth'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5844167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNazTate(naz):\n",
    "    if ',' in naz:\n",
    "        naz = naz.split(',')[1]\n",
    "    if naz == 'Blackheath':\n",
    "        naz= naz.replace('Blackheath', 'United Kingdom')\n",
    "    if naz == 'London':\n",
    "        naz= naz.replace('London', 'United Kingdom')\n",
    "    if naz == 'Kensington':\n",
    "        naz= naz.replace('Kensington', 'United Kingdom')\n",
    "    if naz == 'Chung-hua Min-kuo':\n",
    "        naz= naz.replace('Chung-hua Min-kuo', 'Taiwan')\n",
    "    if naz == 'Solothurn':\n",
    "        naz= naz.replace('Solothurn', 'Schweiz')\n",
    "    if naz == 'Melmerby':\n",
    "        naz= naz.replace('Melmerby', 'United Kingdom')\n",
    "    if naz == 'Montserrat':\n",
    "        naz= naz.replace('Montserrat', 'España')\n",
    "    if naz == 'Canterbury':\n",
    "        naz= naz.replace('Canterbury', 'United Kingdom')\n",
    "    if naz == 'Staten Island':\n",
    "        naz= naz.replace('Staten Island', 'United States')\n",
    "    if naz == 'Epsom':\n",
    "        naz= naz.replace('Epsom', 'United Kingdom')\n",
    "    if naz == 'Wimbledon':\n",
    "        naz= naz.replace('Staten Island', 'United Kingdom')\n",
    "    if naz == 'Plymouth':\n",
    "        naz= naz.replace('Plymouth', 'United Kingdom')\n",
    "    if naz == 'Wimbledon':\n",
    "        naz= naz.replace('Wimbledon', 'United Kingdom')\n",
    "    if naz == 'Edinburgh':\n",
    "        naz= naz.replace('Edinburgh', 'United Kingdom')\n",
    "    if naz == 'Beckington':\n",
    "        naz= naz.replace('Beckington', 'United Kingdom')\n",
    "    if naz == 'Hertfordshire':\n",
    "        naz= naz.replace('Hertfordshire', 'United Kingdom')\n",
    "    if naz == 'Isle of Man':\n",
    "        naz= naz.replace('Isle of Man', 'United Kingdom')\n",
    "    if naz == 'Bristol':\n",
    "        naz= naz.replace('Bristol', 'United Kingdom')\n",
    "    if naz == 'Liverpool':\n",
    "        naz= naz.replace('Liverpool', 'United Kingdom')\n",
    "    if naz == 'Braintree':\n",
    "        naz= naz.replace('Braintree', 'United Kingdom')\n",
    "    if naz == 'Stoke on Trent':\n",
    "        naz= naz.replace('Stoke on Trent', 'United Kingdom')\n",
    "    if naz == 'Rochdale':\n",
    "        naz= naz.replace('Rochdale', 'United Kingdom')\n",
    "    if 'D.C.' in naz:\n",
    "        naz= naz.replace('D.C.', 'Colombia')\n",
    "    if 'Otok' in naz:\n",
    "        naz= naz.replace('Otok', 'Hrvatska')\n",
    "    if 'Département de la' in naz:\n",
    "        naz= naz.replace('Département de la', 'France')\n",
    "    if naz == 'Niederschlesien':\n",
    "        naz= naz.replace('Niederschlesien', 'Polska')\n",
    "    if naz == 'Perth':\n",
    "        naz= naz.replace('Perth', 'Australia')\n",
    "    if naz == 'Bermondsey':\n",
    "        naz= naz.replace('Bermondsey', 'United Kingdom')\n",
    "    if naz == 'Egremont':\n",
    "        naz= naz.replace('Egremont', 'United Kingdom')\n",
    "    if naz == 'Charlotte Amalie':\n",
    "        naz= naz.replace('Charlotte Amalie', 'United States')\n",
    "    if naz == 'Charlieu':\n",
    "        naz= naz.replace('Charlieu', 'France')\n",
    "    if naz == 'Stockholm':\n",
    "        naz= naz.replace('Stockholm', 'Sverige')\n",
    "    if naz == 'Auteuil':\n",
    "        naz= naz.replace('Auteuil', 'France')\n",
    "   \n",
    "    return naz   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e713270",
   "metadata": {},
   "outputs": [],
   "source": [
    "TateArtists[\"placeOfBirth\"] = TateArtists[\"placeOfBirth\"].apply(cleanNazTate)\n",
    "TateArtists.to_csv(\"TateArtists.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "137d67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hrvatska', 'Italia', 'Ceská Republika', 'Belarus', 'België', 'As-Sudan', 'Ísland', 'Taiwan', 'Pakistan', 'Singapore', 'Éire', 'Portugal', 'Bosna i Hercegovina', 'Saint Hélier', 'Shqipëria', 'Myanmar', 'Nihon', 'Perú', 'South Africa', 'Zhonghua', 'România', 'Canada', 'Polska', 'Bangladesh', 'Suomi', 'Al-‘Iraq', 'Mehoz', 'España', 'Jugoslavija', 'Suriyah', 'Eesti', \"Yisra'el\", 'Al-Lubnan', 'Zambia', 'Danmark', 'Schweiz', 'Lietuva', 'Slovenija', 'Tunis', 'Ukrayina', 'Venezuela', 'Uganda', 'Nicaragua', 'Magyarország', 'Misr', 'Schlesien', 'Indonesia', 'México', 'Chile', 'Malta', 'Ellás', 'Sri Lanka', 'Prathet Thai', \"Taehan Min'guk\", 'Samoa', 'Lao', 'New Zealand', 'Slovenská Republika', 'Bénin', 'Malaysia', 'Barbados', 'Mauritius', 'France', 'Cameroun', 'Nigeria', 'Australia', 'Douglas', 'Argentina', 'Kenya', 'Nederland', 'Viet Nam', 'Tanzania', 'Türkiye', 'Moldova', 'Pilipinas', 'Bharat', 'United States', 'Armenia', 'Bahamas', 'Latvija', 'Cuba', 'Makedonija', 'Jamaica', 'Brasil', 'Bulgaria', 'United Kingdom', 'Sverige', 'Guyana', 'Panamá', 'Deutschland', 'Zimbabwe', 'Rossiya', 'Îran', \"Choson Minjujuui In'min Konghwaguk\", 'Norge', 'Luxembourg', 'Costa Rica', 'Österreich', \"Al-Jaza'ir\", 'Colombia'}\n"
     ]
    }
   ],
   "source": [
    "with open('TateArtists.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        nationalities = set() \n",
    "        for item in reader:\n",
    "           \n",
    "            nationality= item['placeOfBirth'].strip()\n",
    "            nationalities.add(nationality)\n",
    "\n",
    "print(nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "91f16049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hrvatska': 9, 'Italia': 80, 'Ceská Republika': 14, 'Belarus': 4, 'België': 37, 'As-Sudan': 1, 'Ísland': 1, 'Taiwan': 1, 'Pakistan': 5, 'Singapore': 2, 'Éire': 51, 'Portugal': 10, 'Bosna i Hercegovina': 2, 'Saint Hélier': 2, 'Shqipëria': 1, 'Myanmar': 1, 'Nihon': 27, 'Perú': 4, 'South Africa': 20, 'Zhonghua': 22, 'România': 13, 'Canada': 40, 'Polska': 42, 'Bangladesh': 2, 'Suomi': 1, 'Al-‘Iraq': 1, 'Mehoz': 2, 'España': 30, 'Jugoslavija': 3, 'Suriyah': 2, 'Eesti': 1, \"Yisra'el\": 11, 'Al-Lubnan': 8, 'Zambia': 1, 'Danmark': 9, 'Schweiz': 30, 'Lietuva': 4, 'Slovenija': 6, 'Tunis': 1, 'Ukrayina': 17, 'Venezuela': 7, 'Uganda': 1, 'Nicaragua': 1, 'Magyarország': 13, 'Misr': 8, 'Schlesien': 2, 'Indonesia': 2, 'México': 13, 'Chile': 5, 'Malta': 1, 'Ellás': 10, 'Sri Lanka': 3, 'Prathet Thai': 1, \"Taehan Min'guk\": 3, 'Samoa': 1, 'Lao': 1, 'New Zealand': 10, 'Slovenská Republika': 3, 'Bénin': 1, 'Malaysia': 1, 'Barbados': 1, 'Mauritius': 2, 'France': 160, 'Cameroun': 1, 'Nigeria': 1, 'Australia': 24, 'Douglas': 1, 'Argentina': 17, 'Kenya': 1, 'Nederland': 35, 'Viet Nam': 2, 'Tanzania': 1, 'Türkiye': 6, 'Moldova': 1, 'Pilipinas': 1, 'Bharat': 24, 'United States': 341, 'Armenia': 1, 'Bahamas': 1, 'Latvija': 3, 'Cuba': 9, 'Makedonija': 1, 'Jamaica': 2, 'Brasil': 30, 'Bulgaria': 2, 'United Kingdom': 1519, 'Sverige': 13, 'Guyana': 2, 'Panamá': 1, 'Deutschland': 142, 'Zimbabwe': 1, 'Rossiya': 32, 'Îran': 10, \"Choson Minjujuui In'min Konghwaguk\": 1, 'Norge': 3, 'Luxembourg': 1, 'Costa Rica': 1, 'Österreich': 29, \"Al-Jaza'ir\": 2, 'Colombia': 10}\n"
     ]
    }
   ],
   "source": [
    "with open('TateArtists.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        TateNationalities ={}\n",
    "        for nationality in nationalities:  \n",
    "            TateNationalities[nationality]= 0\n",
    "        for item in reader:\n",
    "            for naz in nationalities:\n",
    "                    if naz in item['placeOfBirth']:\n",
    "                        TateNationalities[naz] += 1 \n",
    "print(TateNationalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb30d28",
   "metadata": {},
   "source": [
    "#Our analysis.\n",
    "<br>\n",
    "1. What are the most acquired artists in museums (in general)?\n",
    "    - Is there a gender gap in the selection of artists? UOMO-DONNA BASIC\n",
    "    - What are the most represented nationalities (in general)? A TORTA/MENO BASIC MA IL CONCETTO E' QUELLO\n",
    "    - What are the most represented movements or genres (in general)? STESSO COME SOPRA\n",
    "2. How have acquisition criteria changed (over time) in museums?\n",
    "    - In which years are artists' works mostly acquired? BAR CHART/RADIAL BARCHART PER OGNI MUSEO + LINEPLOT PER COMPARARE I 4\n",
    "    - When does the gender gap decreases (if it does)? \n",
    "        COMPUTE PERCENTAGE OF MALE/FEMALE ARTIST ACQUIRED IN TIMESLOTS + COMPARE ACROSS TIME SLOTS \n",
    "        PIECHART/ STACKED BAR CHART WITH ONLY 2 COLORS\n",
    "    - In which years artists' nationalities more influent on the selection? STACKED BAR CHART, SEE TELEGRAM INSPO\n",
    "    - In which years artists' movements/genres more influent on the selection?\n",
    "3. If we compare criteria of all museums, in general and over time, do we see any similarity or significant difference?\n",
    "    - Do certain museums acquire more works based on artists/artists' gender/nationality/movement than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e670d",
   "metadata": {},
   "source": [
    "Acquisition criteria.\n",
    "1.  In which years are artists' works mostly acquired?<br>\n",
    "To answer, we need to count how many times each year shows up in the DateAcquired column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoMa['year'] = pd.DatetimeIndex(MoMa['DateAcquired']).year\n",
    "MoMaNew['DateAcquired'] = MoMa['year']\n",
    "MoMaNew = MoMaNew[MoMaNew['DateAcquired'].notna()]\n",
    "MoMaNew.to_csv(\"MoMaNew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f16c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1922': 115, '1919': 127, '1896': 37, '1916': 37, '1924': 226, '1925': 227, '1927': 225, '1912': 81, '1910': 86, '1909': 50, '1926': 71, '1888': 1060, '1847': 148, '1900': 74, '1940': 207, '1908': 123, '1830': 2, '1877': 11, '1949': 45, '1867': 15, '1905': 64, '1891': 21, '1856': 37893, '1961': 77, '1941': 104, '1957': 51, '1960': 81, '1939': 79, '1946': 128, '1975': 3046, '2008': 787, '2009': 1364, '2010': 392, '2011': 320, '2012': 514, '2013': 458, '1837': 4, '1826': 4, '1823': 1, '1824': 2, '1828': 3, '1835': 1, '1827': 1, '1836': 3, '1841': 2, '1842': 1, '1843': 1, '1849': 1, '1853': 3, '1854': 17, '1857': 2, '1858': 4, '1859': 17, '1860': 2, '1861': 4, '1862': 2, '1863': 1, '1864': 2, '1866': 2, '1876': 3, '1868': 10, '1869': 5, '1870': 1, '1871': 9, '1872': 3, '1873': 1, '1874': 3, '1875': 3, '1834': 1, '1840': 2, '1878': 15, '1879': 49, '1880': 6, '1882': 6, '1883': 3, '1884': 9, '1885': 22, '1886': 9, '1887': 12, '1889': 9, '1890': 8, '1892': 14, '1923': 39, '1893': 9, '1894': 80, '1895': 11, '1897': 37, '1898': 55, '1881': 3, '1906': 17, '1899': 19, '1844': 1, '1901': 9, '1956': 37, '1855': 1, '1902': 12, '1903': 10, '1904': 20, '1907': 35, '1850': 1, '1911': 28, '1913': 25, '1983': 531, '1914': 25, '1915': 24, '1917': 66, '1918': 118, '1920': 41, '1955': 81, '1921': 24, '1982': 597, '1931': 43, '1929': 49, '1928': 38, '1930': 65, '1932': 48, '1933': 50, '1934': 37, '1935': 39, '1936': 31, '1937': 40, '1938': 62, '1942': 53, '1943': 33, '1944': 40, '1945': 42, '1947': 44, '1948': 40, '1951': 48, '1950': 40, '1952': 38, '1953': 58, '1954': 42, '1958': 65, '1964': 84, '1971': 171, '1974': 125, '1972': 279, '1970': 174, '1962': 92, '1976': 607, '1977': 308, '1978': 343, '1979': 1166, '1980': 496, '1967': 131, '1968': 87, '1973': 138, '1981': 380, '1985': 367, '1986': 996, '1987': 218, '1988': 1103, '1989': 355, '1990': 254, '1991': 214, '1984': 241, '1992': 262, '1993': 179, '1994': 275, '1995': 213, '1996': 687, '1997': 3706, '1998': 369, '1999': 290, '2000': 244, '2001': 310, '2002': 319, '2003': 381, '2004': 643, '2005': 333, '2006': 239, '2007': 333, '1959': 92, '1963': 72, '1965': 93, '1966': 86, '1969': 71}\n"
     ]
    }
   ],
   "source": [
    "with open('Tate.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    years={}\n",
    "    for item in reader:\n",
    "        if item['DateAcquired']not in years:\n",
    "            years[item['DateAcquired']]= 1\n",
    "        else:\n",
    "            years[item['DateAcquired']]+= 1\n",
    "\n",
    "    print(years)\n",
    "    #all_years=list(years.keys())\n",
    "    #print(sorted(all_years))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "531b72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1930s': 788, '1940s': 776, '1960s': 1048, '1950s': 633, '1970s': 6853, '1980s': 5538, '1990s': 6693}\n"
     ]
    }
   ],
   "source": [
    "new_dict={}\n",
    "for key in years:\n",
    "    key_int=key.split('.')[0]\n",
    "    key_int=int(key_int)\n",
    "    if key_int in range(1928,1941):\n",
    "        if '1930s' not in new_dict.keys():\n",
    "               new_dict['1930s']= years[key]\n",
    "        else:\n",
    "            new_dict['1930s'] += years[key]\n",
    "    if key_int in range(1940,1951):\n",
    "        if '1940s' not in new_dict.keys():\n",
    "               new_dict['1940s']= years[key]\n",
    "        else:\n",
    "            new_dict['1940s'] += years[key]\n",
    "    \n",
    "    if key_int in range(1950,1961):\n",
    "        if '1950s' not in new_dict.keys():\n",
    "               new_dict['1950s']= years[key]\n",
    "        else:\n",
    "            new_dict['1950s'] += years[key]\n",
    "    \n",
    "    if key_int in range(1960,1971):\n",
    "        if '1960s' not in new_dict.keys():\n",
    "               new_dict['1960s']= years[key]\n",
    "        else:\n",
    "            new_dict['1960s'] += years[key]\n",
    "    \n",
    "    if key_int in range(1970,1981):\n",
    "        if '1970s' not in new_dict.keys():\n",
    "               new_dict['1970s']= years[key]\n",
    "        else:\n",
    "            new_dict['1970s'] += years[key]\n",
    "    if key_int in range(1980,1991):\n",
    "        if '1980s' not in new_dict.keys():\n",
    "               new_dict['1980s']= years[key]\n",
    "        else:\n",
    "            new_dict['1980s'] += years[key]\n",
    "    \n",
    "    if key_int in range(1990,2001):\n",
    "        if '1990s' not in new_dict.keys():\n",
    "               new_dict['1990s']= years[key]\n",
    "        else:\n",
    "            new_dict['1990s'] += years[key]\n",
    "        \n",
    "    \n",
    "print(new_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d03edf",
   "metadata": {},
   "source": [
    "When does the gender gap decreases (if it does)? <br>\n",
    "Per ogni 10 anni, percentuale di uomini e donne acquisiti e differenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MoMaNew.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    years={}\n",
    "    for item in reader:\n",
    "        if item['DateAcquired']not in years:\n",
    "            years[item['DateAcquired']]= 1\n",
    "        else:\n",
    "            years[item['DateAcquired']]+= 1\n",
    "\n",
    "    print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778dda1",
   "metadata": {},
   "source": [
    "When does the gender gap decreases (if it does)?\n",
    "Per ogni 10 anni, percentuale di uomini e donne acquisiti e differenza.\n",
    "\n",
    "In which years artists' nationalities more influent on the selection?\n",
    "Per ogni 10 anni, percentuale di nazionalità acquisite e differenza.\n",
    "\n",
    "In which years artists'movements/genres more influent on the selection?\n",
    "Per ogni 10 anni, percentuale di nazionalità acquisite e differenza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a380e",
   "metadata": {},
   "source": [
    "Nga non ha Gender\n",
    "Met molti Gender sono NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9346281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanWikiIDs(id):\n",
    "    if \"|\" in id:\n",
    "        id = id.split(\"|\")[0]\n",
    "    id = id.split(\"wiki/\")[-1]\n",
    "    id = 'wd:'+id\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c1a886c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46139"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetCleanIds = Met.drop_duplicates(subset='Artist', keep=\"first\")\n",
    "MetCleanIds = MetCleanIds[MetCleanIds[\"Wiki QID\"] != \"|\"]\n",
    "MetCleanIds = MetCleanIds[MetCleanIds[\"Wiki QID\"]!= \"None\"]\n",
    "MetCleanIds = MetCleanIds[MetCleanIds[\"Wiki QID\"]!= \"\"]\n",
    "MetCleanIds[\"Wiki QID\"] = MetCleanIds[\"Wiki QID\"].apply(cleanWikiIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b97ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace , Literal , URIRef\n",
    "from rdflib.namespace import RDF , RDFS\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ssl\n",
    "import rdflib\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# get the endpoint API\n",
    "wikidata_endpoint = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
    "\n",
    "\n",
    "def searchWiki(flag, frame):\n",
    "    artistsIds = frame[\"Wiki QID\"].values\n",
    "    upTo = 500+flag\n",
    "    artistsSlot = artistsIds[flag:upTo]\n",
    "    artists = ' '.join(artistsSlot) \n",
    "\n",
    "    artists_genders_from_ids = \"\"\"\n",
    "    SELECT DISTINCT ?artist ?gender ?genderLabel\n",
    "    WHERE {{\n",
    "      VALUES ?artist {\"\"\"+artists+\"\"\"} .\n",
    "      ?artist wdt:P21 ?gender . \n",
    "      ?gender rdfs:label ?genderLabel .\n",
    "        FILTER (langMatches(lang(?genderLabel), \"EN\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # set the endpoint \n",
    "    sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
    "    # set the query\n",
    "    sparql_wd.setQuery(artists_genders_from_ids)\n",
    "    # set the returned format\n",
    "    sparql_wd.setReturnFormat(JSON)\n",
    "    # get the results\n",
    "    results = sparql_wd.query().convert()\n",
    "\n",
    "    genderDict={}\n",
    "\n",
    "    # manipulate the result\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        artists_id =  'wd:'+result[\"artist\"][\"value\"].split('/')[-1]\n",
    "        if \"genderLabel\" in result: \n",
    "            gender_label = result[\"genderLabel\"][\"value\"]\n",
    "            genderDict[artists_id] = gender_label\n",
    "        else:\n",
    "            genderDict[artists_id] = None\n",
    "    for index, row in frame.iterrows():\n",
    "        for artist_id in genderDict:\n",
    "                if row['Wiki QID']== artist_id:\n",
    "                    frame.at[index,'Gender'] = genderDict[artist_id]\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f916094",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FRANCE~1\\AppData\\Local\\Temp/ipykernel_10260/3535799893.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mieration_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mieration_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msearchWiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMetCleanIds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\FRANCE~1\\AppData\\Local\\Temp/ipykernel_10260/1432172991.py\u001b[0m in \u001b[0;36msearchWiki\u001b[1;34m(flag, frame)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mgenderDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0martists_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0martist_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenderDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Wiki QID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0martist_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;31m# Constructors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     def __init__(\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#finire qui con timer del numero di rows diviso 500, così che si ripeta\n",
    "ieration_num = 500\n",
    "for iter in range(ieration_num):\n",
    "    searchWiki(iter+500, MetCleanIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
